{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MNtgNN3FF1q6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /home/leon/anaconda3/envs/pytorch/lib/python3.10/site-packages (0.7.0)\n",
            "--2024-05-24 17:26:42--  https://web.cs.ucla.edu/~smo3/data.tar.gz\n",
            "Resolving web.cs.ucla.edu (web.cs.ucla.edu)... 131.179.128.29\n",
            "Connecting to web.cs.ucla.edu (web.cs.ucla.edu)|131.179.128.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460347416 (439M) [application/x-gzip]\n",
            "Saving to: ‘data.tar.gz’\n",
            "\n",
            "data.tar.gz         100%[===================>] 439.02M  17.2MB/s    in 21s     \n",
            "\n",
            "2024-05-24 17:27:03 (21.4 MB/s) - ‘data.tar.gz’ saved [460347416/460347416]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "# Downloading this file takes about a few seconds.\n",
        "# Download the tar.gz file from google drive using its file ID.\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "!wget https://web.cs.ucla.edu/~smo3/data.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tB8tvZhIF1q9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "\n",
        "def setup(file_link_dict={},\n",
        "          folder_name='mini_places'):\n",
        "  # Let's make our assignment directory\n",
        "  CS188_path = './'\n",
        "  os.makedirs(os.path.join(CS188_path, 'mini_places', 'data'), exist_ok=True)\n",
        "  # Now, let's specify the assignment path we will be working with as the root.\n",
        "  root_dir = os.path.join(CS188_path, 'mini_places')\n",
        "  # Open the tar.gz file\n",
        "  tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
        "  # Extract the file \"./mini_places/data\" folder\n",
        "  total_size = sum(f.size for f in tar.getmembers())\n",
        "  with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
        "      for member in tar.getmembers():\n",
        "          tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "          pbar.update(member.size)\n",
        "  # Close the tar.gz file\n",
        "  tar.close()\n",
        "  # Next, we download the train/val/test txt files:\n",
        "  for file_name, file_link in file_link_dict.items():\n",
        "      print(f'Downloding {file_name}.txt from {file_link}')\n",
        "      urllib.request.urlretrieve(file_link, f'{root_dir}/data/{file_name}.txt')\n",
        "  return root_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wx-2pvciF1q-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting tar.gz file: 100%|██████████| 566M/566M [00:09<00:00, 58.5MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloding train.txt from https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt\n",
            "Downloding val.txt from https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "val_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt'\n",
        "train_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt'\n",
        "root_dir = setup(\n",
        "    file_link_dict={'train':train_url, 'val':val_url},\n",
        "    folder_name='mini_places')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4MyTph9F1q_"
      },
      "source": [
        "### Define the data transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1v9w2JugF1rA"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "image_net_mean = torch.Tensor([0.485, 0.456, 0.406])\n",
        "image_net_std = torch.Tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "# Notice we are resize images to 128x128 instead of 64x64.\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize([128, 128]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(image_net_mean, image_net_std),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXrmPXGEF1rB"
      },
      "source": [
        "### Define the dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KoHHge5YF1rB"
      },
      "outputs": [],
      "source": [
        "# You can copy your dataset from Assignment2.\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class MiniPlaces(Dataset):\n",
        "    def __init__(self, root_dir, split, transform=None, label_dict=None):\n",
        "        \"\"\"\n",
        "        Initialize the MiniPlaces dataset with the root directory for the images,\n",
        "        the split (train/val/test), an optional data transformation,\n",
        "        and an optional label dictionary.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): Root directory for the MiniPlaces images.\n",
        "            split (str): Split to use ('train', 'val', or 'test').\n",
        "            transform (callable, optional): Optional data transformation to apply to the images.\n",
        "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
        "        \"\"\"\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "\n",
        "        self.label_dict = label_dict if label_dict is not None else {}\n",
        "\n",
        "        with open(f'{root_dir}/{split}.txt', 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            image_path, label = line.split(' ')\n",
        "            label = int(label)\n",
        "            self.filenames.append(image_path)\n",
        "            self.labels.append(label)\n",
        "            if split == 'train':\n",
        "                self.label_dict[label] = image_path.split('/')[-2]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of images in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of images in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return a single image and its corresponding label when given an index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing the image and its label.\n",
        "        \"\"\"\n",
        "        label = self.labels[idx]\n",
        "        image_path = self.filenames[idx]\n",
        "        image = Image.open(os.path.join(self.root_dir, f'images/{image_path}'))\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_th2HTIF1rC"
      },
      "source": [
        "### Define the train method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kfX-uwDDF1rC"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    \"\"\"\n",
        "    Train the MLP classifier on the training set and evaluate it on the validation set every epoch.\n",
        "\n",
        "    Args:\n",
        "        model (MLP): MLP classifier to train.\n",
        "        train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
        "        criterion (callable): Loss function to use for training.\n",
        "        device (torch.device): Device to use for training.\n",
        "        num_epochs (int): Number of epochs to train the model.\n",
        "    \"\"\"\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "\n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the MLP classifier on the test set.\n",
        "\n",
        "    Args:\n",
        "        model (MLP): MLP classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute the accuracy\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "\n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z2Z9TE_UF1rF"
      },
      "outputs": [],
      "source": [
        "def compute_distances_no_loops(x_train, x_test):\n",
        "  num_train = x_train.shape[0]\n",
        "  num_test = x_test.shape[0]\n",
        "  dists = x_train.new_zeros(num_train, num_test)\n",
        "\n",
        "  A = x_train.reshape(num_train,-1)\n",
        "  B = x_test.reshape(num_test,-1)\n",
        "  AB2 = A.mm(B.T)*2\n",
        "  dists = ((A**2).sum(dim = 1).reshape(-1,1) - AB2 + (B**2).sum(dim = 1).reshape(1,-1))**(1/2)\n",
        "  return dists\n",
        "\n",
        "def predict_labels(dists, y_train, k=1):\n",
        "  num_train, num_test = dists.shape\n",
        "  y_pred = torch.zeros(num_test, dtype=torch.int64)\n",
        "\n",
        "  values, indices = torch.topk(dists, k, dim=0, largest=False)\n",
        "  for i in range(indices.shape[1]):\n",
        "    _, idx = torch.max(y_train[indices[:,i]].bincount(), dim = 0)\n",
        "    y_pred[i] = idx\n",
        "  return indices, y_pred\n",
        "\n",
        "class KnnClassifier:\n",
        "  def __init__(self, x_train, y_train):\n",
        "    self.x_train = x_train\n",
        "    self.y_train = y_train\n",
        "\n",
        "  def predict(self, x_test, k=1):\n",
        "    y_test_pred = None\n",
        "\n",
        "    dists = compute_distances_no_loops(self.x_train, x_test)\n",
        "    _, y_test_pred =  predict_labels(dists, self.y_train, k)\n",
        "\n",
        "    return y_test_pred\n",
        "\n",
        "  def check_accuracy(self, x_test, y_test, k=1, quiet=False):\n",
        "    y_test_pred = self.predict(x_test, k=k)\n",
        "    num_samples = x_test.shape[0]\n",
        "    num_correct = (y_test == y_test_pred).sum().item()\n",
        "    accuracy = 100.0 * num_correct / num_samples\n",
        "    msg = (f'Got {num_correct} / {num_samples} correct; '\n",
        "           f'accuracy is {accuracy:.2f}%')\n",
        "    if not quiet:\n",
        "      print(msg)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Pob9aqlkF1rF"
      },
      "outputs": [],
      "source": [
        "# Also, seed everything for reproducibility\n",
        "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dMTQ7y7lF1rF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda. Good to go!\n"
          ]
        }
      ],
      "source": [
        "# Define the device to use for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device == torch.device('cuda'):\n",
        "    print(f'Using device: {device}. Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y67XuOdxF1rG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri May 24 17:27:22 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1650        Off | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   40C    P8               1W /  35W |     58MiB /  4096MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      2785      G   /usr/lib/xorg/Xorg                           37MiB |\n",
            "|    0   N/A  N/A      6031      G   ...local/share/Steam/ubuntu12_32/steam        9MiB |\n",
            "|    0   N/A  N/A      7028      G   ./steamwebhelper                              5MiB |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yktdHkZrKPVw"
      },
      "source": [
        "#### Q1.1.1 Tokenize_image Method (5pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wDPametLF1rJ"
      },
      "outputs": [],
      "source": [
        "def tokenize_image(img, patch_size=16, stride=16):\n",
        "    \"\"\"\n",
        "    Tokenize an image into non-overlapping image patches.\n",
        "    Args:\n",
        "        img (torch.Tensor): The input image with shape (C, H, W).\n",
        "        patch_size (int): The size of each patch.\n",
        "        stride (int): The stride of the sliding window.\n",
        "    Returns:\n",
        "        patches (torch.Tensor): The tokenized patches with shape (N, patch_size*patch_size*C).\n",
        "    \"\"\"\n",
        "    \n",
        "    C, H, W = img.shape\n",
        "    patches = []\n",
        "    # Each patch is flattened into a 1-dimensional vector and stacked into a\n",
        "    # tensor with shape (N, patch_size(H) * patch_size(W) * C), where N is the number of patches.\n",
        "    # We only consider the case image size can be modulo by the patch_size\n",
        "\n",
        "    # Additionally, before flattening, remember to permute the patch such that\n",
        "    # it has shape (patch_size(H), patch_size(W), C)\n",
        "\n",
        "    for row in range(0, H, stride):\n",
        "        for col in range(0, W, stride):\n",
        "            patch = img[:, row:row + patch_size, col:col + patch_size]\n",
        "            patch = torch.permute(patch, (1,2,0))\n",
        "            patch = torch.flatten(patch)\n",
        "            patches.append(patch)\n",
        "\n",
        "    patches = torch.stack(patches)\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sVfIbK5tF1rJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good! For patch_size: 32, the output match\n",
            "Good! For patch_size: 16, the output match\n",
            "Good! For patch_size: 8, the output match\n",
            "Good! For patch_size: 4, the output match\n",
            "Good! For patch_size: 2, the output match\n"
          ]
        }
      ],
      "source": [
        "# test your implementation of tokenize_image\n",
        "random_img = torch.rand(3,64,64)\n",
        "patched_img = tokenize_image(random_img,8,8)\n",
        "\n",
        "for i in [32,16,8,4,2]:\n",
        "    out = tokenize_image(random_img,i,i)\n",
        "\n",
        "    fast_patch = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = i, p2 = i)\n",
        "\n",
        "    answer = fast_patch(random_img.unsqueeze(0))\n",
        "    equal = torch.allclose(out,answer.squeeze(0))\n",
        "    #print('Difference: ', equal)\n",
        "    if equal:\n",
        "      print('Good! For patch_size: %d, the output match' %(i))\n",
        "    else:\n",
        "      print('Uh-oh! For patch_size: %d, the output are different' %(i))\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mZ9tqkaPK1g7"
      },
      "outputs": [],
      "source": [
        "class Tokenization_layer(nn.Module):\n",
        "  def __init__(self, dim, patch_dim, patch_height, patch_width):\n",
        "    super().__init__()\n",
        "    \"\"\"\n",
        "        Args:\n",
        "          dim (int): input and output dimension.\n",
        "          patch_dim(int): falttened vectot dimension for image patch\n",
        "          patch_height (int): height of one image patch\n",
        "          patch_weight (int): weight of one image patch\n",
        "\n",
        "        You can use Pytorch's built-in function and the above Rearrange method.\n",
        "        Input and output shapes of each layer:\n",
        "        1) Rerrange the image: (batch_size, channels, H,W) -> (batch_size,N,patch_dim)\n",
        "        2) Norm Layer1 (LayerNorm): (batch_size,N,patch_dim) -> (batch_size,N,patch_dim)\n",
        "        3) Linear Projection layer: (batch_size,N,patch_dim) -> (batch_size,N,dim)\n",
        "        4) Norm Layer2 (LayerNorm): (batch_size,N,dim)-> (batch_size,N,dim)\n",
        "    \"\"\"\n",
        "\n",
        "    self.to_patch = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n",
        "    self.norm1 = None\n",
        "    self.fc1 = None\n",
        "    self.norm2 = None\n",
        "\n",
        "    # Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n",
        "\n",
        "    self.norm1 = torch.nn.LayerNorm(patch_dim)\n",
        "    self.fc1 = torch.nn.Linear(patch_dim, dim)\n",
        "    self.norm2 = torch.nn.LayerNorm(dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x (torch.Tensor): input tensor in the shape of (batch_size,C,H,W)\n",
        "    Return:\n",
        "      out (torch.Tensor): output patch embedding tensor in the shape of (batch_size,N,dim)\n",
        "\n",
        "     The input tensor 'x' should pass through the following layers:\n",
        "     1) self.to_patch: Rerrange image\n",
        "     2) self.norm1: LayerNorm\n",
        "     3) self.fc1: Fully-Connected layer\n",
        "     4) self.norm2: LayerNorm\n",
        "\n",
        "    \"\"\"\n",
        "    out = self.to_patch(x)\n",
        "    out = self.norm1(out)\n",
        "    out = self.fc1(out)\n",
        "    out = self.norm2(out)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7er82fMhF1rK"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          dim (int): input and output dimension.\n",
        "          heads (int): number of attention heads.\n",
        "          dim_head (int): input dimension of each attention head.\n",
        "          dropout (float): dropout rate for attention and final_linear layer.\n",
        "\n",
        "        Initialize a attention block.\n",
        "        You can use Pytorch's built-in function.\n",
        "        Input and output shapes of each layer:\n",
        "        1) Define the inner dimension as number of heads* dimension of each head\n",
        "        2) to_qkv: (batch_size, dim) -> (batch_size,3*inner_dimension)\n",
        "        3) final_linear: (batch_size, inner_dim) -> (batch_size, dim)\n",
        "        \"\"\"\n",
        "\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "\n",
        "        self.inner_dim = dim_head *  heads\n",
        "\n",
        "\n",
        "        self.attend = None\n",
        "        self.dropout = None\n",
        "        self.final_linear = None\n",
        "\n",
        "        # 1) self.to_qkv: (batch_size, dim) -> (batch_size,3*inner_dimension)\n",
        "        # 2) self.dropout: Dropout layer with ratio defined by dropout variable\n",
        "        # 3) self.final_linear: (batch_size, inner_dim) -> (batch_size, dim)\n",
        "\n",
        "        self.to_qkv = torch.nn.Linear(dim, 3 * self.inner_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.final_linear = torch.nn.Linear(self.inner_dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward pass of the attention block.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
        "\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) to_qkv: (batch_size,N,dim) -> (batch_size,N,3*inner_dimension)\n",
        "        2) Divide the ouput of to qkv to q,k,v and then divide them in to n heads\n",
        "            (batch_size,N,inner_dim) -> (batch_size,N,num_head,head_dim)\n",
        "        3) Use torch.matmul to get the product of q and k\n",
        "        4) Divide the above tensor by the squre root of head dimension\n",
        "        5) Apply softmax and then dropout on the above tensor\n",
        "        6) Mutiply the above tensor with v to get attention\n",
        "        7) Concatenate the attentions from multi-heads\n",
        "            (batch_size,N,num_head,head_dim) -> (batch_size,N,inner_dim)\n",
        "        8) Pass the output from last step to a fully connected layer\n",
        "        9) Apply dropout for the last step output\n",
        "        '''\n",
        "        out = None\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "        # This operation will change the tensor shape from (batch_size,N,num_head,head_dim)\n",
        "        # to  (batch_size,N,inner_dim)\n",
        "        out = q @ k.transpose(-2, -1) / (self.dim_head ** 0.5)\n",
        "        out = self.dropout(torch.nn.functional.softmax(out, dim=-1))\n",
        "        out = out @ v\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.dropout(self.final_linear(out))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G7yya6zjF1rL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good! For input dim: 512, the output shape is correct\n",
            "Good! For input dim: 768, the output shape is correct\n",
            "Good! For input dim: 1096, the output shape is correct\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check if the output shape of attention'\n",
        "for dim in [512,768,1096]:\n",
        "  test_tensor = torch.rand(2,196,dim)\n",
        "  att_layer = Attention(dim,8,64,0.4)\n",
        "  output_tensor = att_layer(test_tensor)\n",
        "  equal =  test_tensor.shape == output_tensor.shape\n",
        "  if equal:\n",
        "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
        "  else:\n",
        "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRGAesmOfk0A"
      },
      "source": [
        "The norm layer in Vision Transformer (ViT) is a layer that performs layer normalization on the input. It is typically applied after the Multi-Head Attention (MHA) and the MLP layers in the ViT architecture. The norm layer is used to help the model learn better representations by ensuring that the activations are normalized and centered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jFbmcWQCF1rL"
      },
      "outputs": [],
      "source": [
        "### PreNorm function\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        # keey the residual connection here\n",
        "        return self.fn(self.norm(x), **kwargs)+x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "l217s0ofF1rL"
      },
      "outputs": [],
      "source": [
        "#You can use\n",
        "a = PreNorm(768, Attention(768, heads = 8, dim_head = 64, dropout = 0.2))\n",
        "# to create a combination of layer norm and any other layer\n",
        "test_tensor = torch.rand(2,196,768)\n",
        "# you can use the following line to do the forward pass\n",
        "output_tensor = a(test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zP9qXq3AF1rM"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, dim, mlp_dim, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        \"\"\"\n",
        "         Args:\n",
        "          dim (int): input and output dimension.\n",
        "          mlp_dim (int): the output dimension of the first layer.\n",
        "          dropout (float): dropout rate for both linear layers.\n",
        "\n",
        "        Initialize an MLP.\n",
        "        Input and output sizes of each layer:\n",
        "          1) fc1: dim, mlp_dim\n",
        "          2) fc2: mlp_dim, dim\n",
        "        \"\"\"\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.dropout = None\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(dim, mlp_dim)\n",
        "        self.fc2 = torch.nn.Linear(mlp_dim, dim)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
        "\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) fc1: (batch_size,N,dim) ->  (batch_size,N,mlp_dim)\n",
        "        2) Apply activation function\n",
        "        3) Apply dropout\n",
        "        3) fc2: (batch_size,N,mlp_dim) -> (batch_size,N,dim)\n",
        "        4) Apply dropout\n",
        "        '''\n",
        "\n",
        "        out = self.fc1(x)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0o1D-3EsF1rM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good! For input dim: 512, the output shape is correct\n",
            "Good! For input dim: 768, the output shape is correct\n",
            "Good! For input dim: 1096, the output shape is correct\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check if the output shape of PositionwiseFeedForward\n",
        "for dim in [512,768,1096]:\n",
        "  test_tensor = torch.rand(2,196,dim)\n",
        "  ffn = PositionwiseFeedForward(dim,dim*4,0.1)\n",
        "  output_tensor = ffn(test_tensor)\n",
        "  equal =  test_tensor.shape == output_tensor.shape\n",
        "  if equal:\n",
        "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
        "  else:\n",
        "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoRkYfdoF1rM"
      },
      "source": [
        "1. Apply Layer-norm to the input tensor\n",
        "2. Apply the Multi-Head Attention (MHA) layer to the output tensor from step1. The MHA layer takes in the input tensor, and returns the attention scores and the attention output tensor.\n",
        "3. Add the residual connection to the output of the MHA layer.\n",
        "4. Apply Layer-norm to output of last step\n",
        "5. Apply the Position-wise Feedforward Network (FFN) layer to the output of the previous step. The FFN layer takes in the output tensor, and returns the transformed output tensor.\n",
        "6. Add the residual connection to the output of the FFN layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lMCLL9IJF1rN"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        \"Implements Transformer block.\"\n",
        "        super().__init__()\n",
        "        '''\n",
        "        Args:\n",
        "          dim (int): input and output dimension.\n",
        "          heads (int): number of attention heads.\n",
        "          dim_head (int): input dimension of each attention head.\n",
        "          mlp_dim (int):\n",
        "          dropout (float): dropout rate for attention and FFN layers.\n",
        "\n",
        "        '''\n",
        "        # Use the PreNorm,Attention and PositionwiseFeedForword class to build your\n",
        "        # Transformer block\n",
        "\n",
        "        self.attn = PreNorm(dim, Attention(dim, heads, dim_head, dropout))\n",
        "        self.ff = PreNorm(dim, PositionwiseFeedForward(dim, mlp_dim, dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
        "        \"\"\"\n",
        "        x = self.attn(x)\n",
        "        x = self.ff(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XLAFJkurF1rN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good! For input dim: 512, the output shape is correct\n",
            "Good! For input dim: 768, the output shape is correct\n",
            "Good! For input dim: 1096, the output shape is correct\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check if the output shape of Transformer\n",
        "for dim in [512,768,1096]:\n",
        "  test_tensor = torch.rand(2,196,dim)\n",
        "  transformer_block = Transformer(dim,8,64,dim*4,0.1)\n",
        "  output_tensor = transformer_block(test_tensor)\n",
        "  equal =  test_tensor.shape == output_tensor.shape\n",
        "  if equal:\n",
        "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
        "  else:\n",
        "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8h8oMh2vF1rN"
      },
      "outputs": [],
      "source": [
        "# helper method\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"Implements Vision Transfromer\"\n",
        "    def __init__(self, *,\n",
        "                 image_size,\n",
        "                 patch_size,\n",
        "                 num_classes,\n",
        "                 dim,\n",
        "                 depth,\n",
        "                 heads,\n",
        "                 mlp_dim,\n",
        "                 pool = 'cls',\n",
        "                 channels = 3,\n",
        "                 dim_head = 64,\n",
        "                 dropout = 0.,\n",
        "                 emb_dropout = 0.,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          image_size (int): the height/weight of the input image.\n",
        "          patch_size (int): image patch size. In the ViT paper, this value is 16.\n",
        "          num_classes (num_class): Number of image classes for MLP prediction head.\n",
        "          dim (int): patch and position embedding dimension.\n",
        "          depth (int): number of stacked transformer blocks.\n",
        "          heads (int): number of attention heads.\n",
        "          mlp_dim (int): inner dimension for MLP in transformer blocks.\n",
        "          pool (str): choice between \"cls\", \"mean\", \"none\".\n",
        "                      For cls, you will need to use the cls token for perdiction\n",
        "                      For mean, you will need to take the mean of last transformer output\n",
        "                      For none, you can just return the last transformer output.\n",
        "                                This will mainly be used for dense prediction tasks.\n",
        "          channels (int): Input image channels. Set to 3 for RGB image.\n",
        "          dropout (float): dropout rate for transformer blocks.\n",
        "          emb_dropout (float): dropout rate for patch embedding.\n",
        "        \"\"\"\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = 0\n",
        "        patch_dim = 0\n",
        "\n",
        "        # TODO: Compute the num_patches and patch_dim\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = patch_size * patch_size * channels\n",
        "\n",
        "        assert pool in {'cls', 'mean', 'none'}, 'pool type must be either cls (cls token), mean (mean pooling), or none (no pooling)'\n",
        "        self.pool = pool\n",
        "\n",
        "        self.to_patch_embedding = None\n",
        "\n",
        "        self.pos_embedding = None\n",
        "        self.cls_token = None\n",
        "        self.dropout = None\n",
        "        self.transformers = nn.ModuleList([])\n",
        "        self.mlp_head = None\n",
        "\n",
        "        # 1) Define self.to_patch_embedding usinng the Tokenization_layer class\n",
        "        # 2) Define learnable 1-D pos_embedding using torch.randn, the number of\n",
        "        #    embedding should be num_patches+1\n",
        "        # 3) Define learnable 1-D cls_token with dimension = dim. You can use\n",
        "        #    nn.Parameter and torch.randn to initialize this\n",
        "        # 4) Define dropout with emb_dropout\n",
        "        # 5) Define array of d Transformer modules, where d=depth\n",
        "        # 6) Using nn.Sqeuential to create the MLP head including two layers:\n",
        "        #    The first layer in the MLP head is a LayerNorm layer.\n",
        "        #    The second layer in the MLP head is a linear layer change dimension to num_classes\n",
        "        #    Note that this MLP head should have 'dim' input dimensions, not 'mlp_dim' which is\n",
        "        #    used for the MLP in the transformer block instead.\n",
        "\n",
        "        self.to_patch_embedding = Tokenization_layer(dim, patch_dim, patch_height, patch_width)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "        self.transformers = nn.ModuleList([\n",
        "            Transformer(dim=dim, heads=heads, dim_head=dim_head, mlp_dim=mlp_dim, dropout=dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        '''\n",
        "        Args:\n",
        "            img (torch.Tensor): input tensor in the shape of (batch_size,C,H,W).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,num_class).\n",
        "\n",
        "        The input tensor 'img' should pass through the following layers:\n",
        "        1) self.to_patch_embedding: (batch_size,C,H,W) -> (batch_size,N,dim)\n",
        "        2) Using torch.Tensor.repeat to repeat the cls alone batch dimension.\n",
        "           Then, concatenate with cls token (batch_size,N,dim) -> (batch_size,N+1,dim)\n",
        "        3) Take sum of patch embedding and position embedding, then apply dropout.\n",
        "        4) Passing through all the transformer blocks (batch_size,N+1,dim) -> (batch_size,N+1,dim)\n",
        "        5) If pool is none, simply return the output of (4). Else, proceed to (5).\n",
        "        5) Use cls token or use pool method to get latent code of batched images\n",
        "            (batch_size,N+1,dim) -> (batch_size,dim)\n",
        "        6) Apply MLP head to the output of last step: (batch_size,dim) -> (batch_size,num_class)\n",
        "\n",
        "        '''\n",
        "\n",
        "        x = self.to_patch_embedding(img)\n",
        "        cls_token = self.cls_token.repeat(x.shape[0], 1, 1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.dropout(x)\n",
        "        for transformer in self.transformers:\n",
        "            x = transformer(x)\n",
        "\n",
        "        if self.pool == 'none':\n",
        "            return x\n",
        "\n",
        "        if self.pool == 'cls':\n",
        "            x = x[:, 0, :]\n",
        "        elif self.pool == 'mean':\n",
        "            x = x.mean(dim=1)\n",
        "\n",
        "        out = self.mlp_head(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8oNroxoyG1r"
      },
      "source": [
        "Then let's train your ViT model with with cls token as pool policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Tbp_mTZJF1rO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2: 100%|██████████| 1563/1563 [03:01<00:00,  8.61it/s, loss=4.04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 3.7702, Accuracy = 0.1217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [03:02<00:00,  8.59it/s, loss=3.15]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 3.5794, Accuracy = 0.1477\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "#Define the model, optimizer, and criterion (loss_fn)\n",
        "model = ViT(image_size = 128,\n",
        "    patch_size = 16,\n",
        "    num_classes = 100,\n",
        "    dim = 192,\n",
        "    depth = 8,\n",
        "    heads = 4,\n",
        "    dim_head = 48,\n",
        "    mlp_dim = 768,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        "           )\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(                                                                                   \n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RWLFLdbwy3ko"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2: 100%|██████████| 1563/1563 [03:03<00:00,  8.52it/s, loss=3.98]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 3.7936, Accuracy = 0.1174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [03:03<00:00,  8.53it/s, loss=3.17]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 3.5827, Accuracy = 0.1470\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "#Define the model, optimizer, and criterion (loss_fn)\n",
        "model = ViT(image_size = 128,\n",
        "    patch_size = 16,\n",
        "    num_classes = 100,\n",
        "    dim = 192,\n",
        "    depth = 8,\n",
        "    heads = 4,\n",
        "    pool = 'mean',\n",
        "    dim_head = 48,\n",
        "    mlp_dim = 768,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        "           )\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PzBbgHQO2DF7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leon/anaconda3/envs/pytorch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/leon/anaconda3/envs/pytorch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/2: 100%|██████████| 1563/1563 [03:29<00:00,  7.46it/s, loss=3.57]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 3.1604, Accuracy = 0.2222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [03:05<00:00,  8.41it/s, loss=2.64]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set: Average loss = 2.8054, Accuracy = 0.2912\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "resnet = models.resnet18(pretrained=False, num_classes=100) # Resnet(pretrained=False)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    resnet.parameters(),\n",
        "    lr=0.0001)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(resnet, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "X42ej8ZhF1q4"
      ],
      "gpuClass": "premium",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
